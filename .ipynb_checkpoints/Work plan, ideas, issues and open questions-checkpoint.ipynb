{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e26d7c9",
   "metadata": {},
   "source": [
    "<a id='back_to_top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b86bf2",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "- ### [Work Plan](#work_plan)\n",
    "    - [Phase 1](#work_plan)\n",
    "    - [Phase 2](#phase_2)\n",
    "- ### [Observations about the Data](#observations)\n",
    "    - [The following features should be related](#features_related)\n",
    "    - [Comments from \"Exploratory Data Analysis of the historical features - Part 1\"](#comments_part_1)\n",
    "- ### [Ideas](#ideas)\n",
    "- ### [Issues](#issues)\n",
    "- ### [Open Questions](#questions)  \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4f035",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='work_plan'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e5c9a",
   "metadata": {},
   "source": [
    "# Work Plan \n",
    "(in order of projectd progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3e84e",
   "metadata": {},
   "source": [
    "## Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f5b59",
   "metadata": {},
   "source": [
    "- #### Estimate the relations between the variables\n",
    "    - Examine the relations between the different variables by calculating the appropriate correlations and mutual-information tests, depending on the features' type (nominal, ordinal or numerical). \n",
    "    - For example, there are different statistical correlations tests for ordinal-numerical and numerical-numerical feature pairs; and different tests for numerical features that their values are normally, or non-normally, distributed.\n",
    "    - Mutual Information: learn how to use the MI for the different data types; and what are the relevant validity tests.\n",
    "    - Explain inside the notebooks why each test is done for each feature pair. \n",
    "    - Perform the relevant validity tests for each correlations algo. For example, check for normality of the data. \n",
    "   \n",
    "    \n",
    "- #### Zeros \n",
    "    - The above refers to running the tests over all the data. However, for almost all features, the majority of their values is zeros. This in turn may strongly influence, or distort, the relational results between the features, and between the features and the target. In particular, it may point out for significance where non exist.\n",
    "    - In addition, in some cases the zero values in one feature is trivially determined by the other. For example, if the user's recency is above 30 days, then their net revenue in the last 30 days is guaranteed to be 0 as well.\n",
    "    - Task: run the correlations/mutual-information tests for only the non-zero values' pairs of each two features (or feature - target). \n",
    "    \n",
    "    \n",
    "- #### Summary of this section \n",
    "    - for each feature pair and for each feature-target pair, there should be at the end the correlation coefficient, p-value and whether the data keeps with the test validity conditions. For the two cases of considering all the values, and for considering only the non-zero values' pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f47e53",
   "metadata": {},
   "source": [
    "- #### Kmeans\n",
    "    - Learn how to correctly implement Kmeans for mixes of data types. For example, what pre-processing steps are required (e.g., normalize the data), and how such steps could be implemented on ordinal or categorical feature types? (e.g., how do categorical or nominal data can be normalized?) \n",
    "    - Understand the impact of the zero-valued imbalanced data on the clustering results\n",
    "    - Initially consider clustering of two or three features, and examine by visualization the resulted 2- and 3-d clusters. Especially examine whether the clusters are separated, or actually form clouds of data that is only divided into clusters due to the way the kmeans work (number of clusters is pre-determined).\n",
    "    - Run cluster quality tests to determine what is the optimal number of clusters is, and how well the clusters are separated.\n",
    "    - Run density clustring algorithms (like DBSCAN) to confirm the observed structure (if indeed there are, e.g., only two cluster clouds that are artificially divided into 10 clusters because of the Kmeans, a density clustering algorithm like DBSCAN should result with only two clusters). \n",
    "    - Examine the impact of the zeros imbalanced data on the clustering results.\n",
    "    - Examine the variability in the clusters size. Could be one huge cluster of all zeros, and few smaller clusters built from the non-zero values sets.  \n",
    "\n",
    "    - Outliers filtering: a step in the algo that is currently in production aims at filtering out outliers by doing the following - \n",
    "        - Running DBSCAN on the data \n",
    "        - Find clusters that are deemed as outliers (probably by size, or by being at the extreme ends of some features values)\n",
    "        - Filter them out by setting cut-off values to those features' values. \n",
    "        - For example (this is my own example for clarification purposes. I don't know what are the parameters in the production), if 99% of a given feature are between 0 and 100, and the remaining 1% is at values above 1000, then setting a cut-off at 110 will get rid of these outliers. \n",
    "        - Comment: I am not sure I understand the logic of running DBSCAN just for finding outliers, and then running Kmeans to segment the data. As mentioned above, running a density algorithm seems like a better solution than running Kmeans for segmentation purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36cb90",
   "metadata": {},
   "source": [
    "- #### Get initial homogeneity results \n",
    "    - With the above done, it will be possible to reach informative initial results from running the Kmeans on the data under different set-ups (in particular, with or without the zeros valued data points), and calculate the within-cluster (aka within-segment) distribution of the target KPI. \n",
    "    - Suggest an initial homogeneity function. \n",
    "    - Examine how the homogeneity outcome is modified when considering different features sets for the clustering. For example, take the features that are mostly correlated (or have the highest mutual information) with the target; or take the features that have the highest entropy in the data set; or take the features that are mostly un-correlated within the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090c001",
   "metadata": {},
   "source": [
    "- #### Summary of Phase 1 - \n",
    "    - Get data of users features of D-1 + target KPI data of D-0\n",
    "    - Select optimal features set (where optimal is determined by correlations/entropy/mutual-information/other) \n",
    "    - Do Kmeans on all the D-1 data (make sure to do the Kmeans in the correct way).\n",
    "    - Default number of clusters is 10, but do clustering analysis to determine the the optimal clustersâ€™ number analysis, and run other performance tests and analyze the results.\n",
    "    - Examine the impact of the zeros imbalanced data on the clustering results.\n",
    "    - Examine the variability in the clusters size. Could be one huge cluster of zeros, and few smaller clusters built from the non-zero values sets.\n",
    "    - For the users that appears in the D-1 data and the D-0 target, calculate the homogeneity factor. That is, examine the distribution of the target values within and in between the clusters.\n",
    "    - In addition, examine the distribution of target users between the clusters - as the data is highly imbalanced (e.g., much more non-vip users, and much more non-paying users), it could be that there will be clusters with few to no target users in them (e.g., cluster for users that deposit above 1000$, so it is a relatively much smaller cluster than the others, and so no user from this cluster that appeard in the D-1 data set appears in the D-0 data set). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563d303",
   "metadata": {},
   "source": [
    "- #### Comments for Phase 1 - \n",
    "    - Given the nature of the problem (as discussed already with Andrey, this project aims at finding the best/optimal set of features for a given target KPI by employing some logic as a pre-processing step before the un-supervised clustering step), it is not clear what logic to consider for choosing the best features-set for a given target KPI. \n",
    "    - The hope is that through a bit of search and guess, I should have an idea of what might work. That is, consider the features with the highest correlations with the target KPI? the highest mutual information with the target KPI? some other logic?\n",
    "    - And so for now a new set of features, and a new target KPI, run this logic to determine the feature-set on which the segmentation is done, such that the target KPI is distributed most homogenously within a cluster (and also between clusters). \n",
    "    - Homogeneity function: \n",
    "        - Suggest a homogeneity metrics \n",
    "        - Make it intuitive for business end-users. For example, by limiting it to be between 0 and 1, such that 0.8 is understandably a much better result than 0.2. \n",
    "        \n",
    "    - Show the results for several target KPIs. \n",
    "    - Do so also by choosing two or three features, such that visualization of the clustering is possible, and it is easier to explain the results. \n",
    "    - Benchmark: compare different logics for choosing the feature-set for segmentation, and set a benchmark for the homogeneity by considering a random choice of features; by correlations; by mutual-information, etc., and see if any logic is significantly better than the others. \n",
    "    - This benchmark will also be used to evaluate the added value of the steps in Phase 2, e.g., the PCA step and the supervised approach. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8100112",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='phase_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67831f26",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564df15",
   "metadata": {},
   "source": [
    "- #### Dimensionality reduction\n",
    "    - Given the results of Phase 1, consider dimensionality reduction pre-processing steps, and then choose the most informative principal components for the clustering (segmentation) step. \n",
    "    - This may improve the clusters' properties, and especially on such data set that has many features that are co-dependent. \n",
    "    - Still, such a step may reduce the interpretability of the results. Thus, any potential improvement in the results should be weighed against this reduction. \n",
    "    - Things to do: \n",
    "        - Examine how to adapt the PCA algorithm to the mixed data types, and to a zeros-imbalanced data set. \n",
    "        - Are pre-process steps required before running the PCA? for example, data normalization? (If so, how one normalizes ordinal or categorical feature types?)\n",
    "        - What other dimensionality reduction approaches are there to consider? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dd951",
   "metadata": {},
   "source": [
    "- #### Visualization \n",
    "    - Examine what visualization approaches can be used for when more than three features are used for the clustering step. For example, tSNE. Other approaches exist, and as much as I remember than might be better than tSNE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5f1f1",
   "metadata": {},
   "source": [
    "- #### Supervised segmentation \n",
    "    - The above is based on examining the degree of intra- and inter-cluster homogeneity when performing unsupervised data-segmentation, with the possibility of improving the homogeneity by choosing 'better features' for the clustering steps. \n",
    "    - 'better features' here means, e.g., features that have a higher correlations with the target KPI, or features that have lower correlations between them, or choosing the most informative principal components of some dimensionality reduction approach. \n",
    "    - An alternative approach (and in my opinion the prefered one) is to perform **supervised data-segmentation**. \n",
    "    - For example, run CatBoost on the users that appears both in the data and the target, then find which features are the most important for the results (classification or regression, depending on the properties of the target).\n",
    "    - Then, segment the users as follows: determine how many segments are required, and decide which target values define each segment. For example, if the target is revenue, then possible segments could be [0$, 1$], [1$, 10$], [10$, 100$], and so on. And then, find what is the average values of the set of important features for each segment. This will determine the 'clusters' centers. \n",
    "    - For each user, calculate their distance from the clusters centers, and match the user to the cluster their distance is the lowest. And with that, the clusters, or segments, will get populate.  \n",
    "    - Note: for this approach to be practical, I will need to examine how fast CatBoost (or any other such model) can end its learning phase over 0.5 and 1 million users. It is supposed to be a few seconds, but I will need to check it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8702cf3",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='observations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43997a",
   "metadata": {},
   "source": [
    "# Observations about the Data  <a id='features_related'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbe4e3",
   "metadata": {},
   "source": [
    "## The following features should be very much related: \n",
    "  \n",
    "\n",
    "- **is_elite, is_vip** - If a user is vip then it is also elite (but given the numbers of users in each category, being an elite user doesn't mean they are also vip). Also, if a user is elite/vip, their revenue and number of transactions is higher than the average user (practically by defintion). <font color='red'>Still need to confirm this -> that there is an overlap between the elite and vip. The alternative is that there are three distinct classes of users - not elite or vip, elite, and vip. </font>\n",
    "  \n",
    "  \n",
    "- **Lifetime_Revenue_Net, Last_30D_Revenue_Net, Last_90D_Revenue_Net, Last_30D_Transactions** (correlation coefficients between this variables are indeed quite high). \n",
    "\n",
    "\n",
    "- **level, level_bracket** - if a user has a higher level, than it is also at a higher level bracket.   \n",
    "<font color='red'>Still need to confirm this in the analysis. </font>\n",
    "\n",
    "\n",
    "- **credits_balance_EOD, coins_balance_EOD** - There should also be relation to **level and level bracket**, that is, the higher the balance the higher the user level is. <font color='red'>Still need to confirm this latter assertion in the analysis. </font>\n",
    "\n",
    "\n",
    "- **Last_30D_Gross_Sink, Last_90D_Gross_Sink** (analysis confirm this). \n",
    "\n",
    "\n",
    "- **Last_30D_Login_days, Last_90D_Login_days** - Ca. **70%** of the users has **not login** even once in the **last 30 days**, and ca. **60%** has **not login** even once in the **last 90 days**. For all of these users, all of the features that refer to this time window (e.g., Last_30D_Revenue_Net or Last_90D_Gross_Sink) have, by definition, zero value (as zero logins means zero revenue or sink). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9fdbf",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='comments_part_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc430939",
   "metadata": {},
   "source": [
    "## Comments from \"Exploratory Data Analysis of the historical features - Part 1\"\n",
    "\n",
    "1. **Selecting features by their entropy**: \n",
    "    - The entorpy of many of the features is very low. For example, the 'is_vip' feature -> almost all of the users are not VIP. \n",
    "    - Other such features are 'is_elite', and the revenue features (in the last 30 days, ca. 94% of the users have zero revenue in that time period). \n",
    "    - Such features most probably doesn't contribute to the segmentation, and so they should be ignored (but see pt.3 below). \n",
    "    - That is, if all features had a uniform distribution (maximal entropy) over the users, it would have been easier to segment the users into evenly sized seperate groups. However, with the distributions of the users for almost all the features being highly skewed (e.g., very few users at the high level brackets, with the vast majority of the users at the lower brackets), it is not clear how to reach a segmentation beyond the trivial clusters determined by the features distributions. \n",
    "    - This is especially the case, as many of the features are highly correlated, or at least highly related: vip users are also those with the higher revenues and at the higher level brackets, practically by definition. \n",
    "    - <font color='red'>Such feature selection implicitly takes place when performing dimensionality reduction on the feature space, like with **PCA** (**any other approach?**). So performing PCA and then choosing only the few most significant principles components will filter out the less informative features.</font> \n",
    "    - <font color='red'>Still to check: how long does it takes to do PCA on a 5x10^6 x 20 feature table, and if it takes very long, find approximate solution -> perform the PCA on a smaller data set, or simply drop features with low entropy (like the 'is_vip' feature).</font>    \n",
    "    \n",
    "\n",
    "2. **Obvious segments without even clustering or any other techniques**: \n",
    "    - Some of the features that has very low entropy create immidate segments. For example, if the target function is revenue, then the VIP users, although relatively very few from the overall population, represent a segment of their own. \n",
    "    - This is of course trivial -> most probably a user recieves the VIP status because of high frequency of deposits -> this can greatly simplify the segmentation step in many cases. As in, after the trivial segments are built, other segments can be estimated from clusters on raw data that doesn't include these trivial features.  \n",
    "    \n",
    "    \n",
    "3. **Segmentation with the target KPI in mind**: \n",
    "    - If the target KPI is **revenue**, then there are several issues that stems from that this is a **very sparse KPI**. \n",
    "    - That is, most of the users in the data-set has zero past revenue, and will have zero revenue in the future. \n",
    "    - In addition, there are very few features, if at all, that provide a good prediction to whether a user will make future deposits (as obvious from that in the LTV project no model exist for predicting the future revenue at the user level, and no such model exist for active users). \n",
    "    - So most possible the best segmentation in this case of revenue as the KPI (especially if it has a short horizon of a few days), is to take the naive segmentation of two groups: the first, is of \"all users that deposited in the past\", and the second is \"all users that had no past deposits\". The first (much smaller) segment could be further segmented into smaller groups. \n",
    "    - In addition, it is predicted that adding features (that are not revenue related) to the segmentation would only reduce the homogeniety of the segments. \n",
    "    - Thus, this is a clear case of an obvious feature-set to consider for the segmentation for a specific target KPI, if the homogeneity is to be optimized. Any other more sophisticated algorithm will most probably fail. <font color='red'>**And so the question is: what is the view of the product/studio for such a case?**</font>\n",
    "    - to check: most probably running a CatBoost on the overall raw past feature set will result with the revenue features being the most important ones to predict future revenue, in-line with using such a model as a general solution for the task at hand.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e4630",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='ideas'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17923824",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8da1c",
   "metadata": {},
   "source": [
    "### Possible things to do: \n",
    "\n",
    "- <font color='blue'>Randomly select a few sets of three features, and for each set cluster the users (10 clusters using Kmeans), and examine whether the homogeneity of the target KPI values is significantly different for the different clusters' sets.</font>\n",
    "\n",
    "\n",
    "- <font color='blue'>It could be that with random initialization of the cluster centers, relatively small clusters of users are being missed (due to the imbalance of the data in almost all the features).</font> I should be able to check this with a few simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1ed6a",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='issues'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb40a4",
   "metadata": {},
   "source": [
    "# Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa9392",
   "metadata": {},
   "source": [
    "[back to top](#back_to_top)  <a id='questions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbddabc",
   "metadata": {},
   "source": [
    "# Open Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56009784",
   "metadata": {},
   "source": [
    "(answers in blue follows discussion with Andrey)\n",
    "\n",
    "- <font color='red'>**The meaning of the targets**: \n",
    "- 'Revenue' means the new total revenue of a user for the date of the target query? so the total revenue in 1.10.2022 (when the segmentation/clustering is done at) was 10 dollar, and we want to check for a target of revenue after 10 days, so if the revenue on the 10.10.2022 is 14 dollar, it means that in this 10 days the user deposited 4 dollar in total?</font>  \n",
    "<font color='blue'>Answer: The target revenue is the netto revenue of a given user for the given dates the target query was for. </font>\n",
    "   \n",
    "\n",
    "- <font color='red'> 'Level Ups' means the number of levels a user was up in the period between the query for the clustering and the query for the target? </font>   \n",
    "<font color='blue'>Answer: the netto number of levels a user was up for the certain range of dates the target query was for. Users can only go up in level number.</font>     \n",
    "  \n",
    "\n",
    "- <font color='red'>'Mode Number' means the current mode of the user, and the purpose is to see if by the time of the target query it went up or down? (can the mode number go down, or can it only go up?) </font>     \n",
    "<font color='blue'>Answer: Mode Number is the decision of a user in trying to change levels, or make a game. In a given day, a user can do many rounds, some of them in mode 2 and others in mode 3, and so on. There is no constraint on that.\n",
    "Studios aim at detecting users that play at a certain mode, in order to target them for certain game features (as users that play at higher modes spend their coins at a higher rate).</font>\n",
    "\n",
    "\n",
    "- <font color='red'>There are users with **negative 'Lifetime_Revenue_Net'**. Need to check if it is a bug, and if not, how to treat such users.    \n",
    "<font color='blue'>Answer: it is not known. At the moment, it is Ok to ignore these users (whom are very few of the whole set, i.e., 15k out of 5 million). </font>\n",
    "\n",
    "\n",
    "- <font color='red'>Is **coins and credits balance end-of-day** is the last value a user had for these features? (so, user 872 had 12 coins and 78 credits as she left the app the last time she used it)  \n",
    "<font color='blue'>Answer: it is the value of the day of the query (and not some historical sum or average like for the revenue featutre). </font>\n",
    "\n",
    "\n",
    "- <font color='red'>What does the **number of bingo rounds** refers to? the average rounds a user play per day? the number of rounds a user had at their last time playing the app? some other value? \n",
    "<font color='blue'>Answer: most probably lifetime number of bingo rounds a user played during their time at the game.</font>\n",
    "\n",
    "\n",
    "- <font color='red'>**trstier** - how this feature is defined? and from its definition, it will be clearer how it is related to other features, such as level, level bracket, credit balance, coin balance, is vip and is elite.  \n",
    "<font color='blue'>Answer: Rarely updates, after a user reaches some milestones (e.g., regarding deposits or time of play and so on). It is not common that a user goes down tier level. Related therefore to many of the other features such as level bracket, level, revenue. </font> \n",
    "\n",
    "\n",
    "- <font color='red'>What do the **modes** 1 to 6 refers to? (in the total_rounds_mode_X features)   \n",
    "<font color='blue'>Answer: a user can play at a given mode each time they play the app. The higher the mode the more the user spend in the game (e.g., coins).</font> \n",
    "\n",
    "\n",
    "- <font color='red'>When segmenting, is there a goal/constraint to have segments with similar number of users? or can it be that one segment has million users, and the other only 10?   \n",
    "<font color='blue'>Answer: No a-priori constrain on the number of users per clusters. But anyone who runs the clustering algo have the option to choose that there will be a minimal or/and maximal number of users in a cluster. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b945418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
